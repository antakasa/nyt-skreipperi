{"nbformat_minor": 0, "cells": [{"execution_count": 3, "cell_type": "code", "source": "from nytimesarticle import articleAPI\napi = articleAPI('c3f15f0c26afcc04fe4d09d0bb7525a9:4:71581877')\ndef parse_articles(articles):\n    '''\n    This function takes in a response to the NYT api and parses\n    the articles into a list of dictionaries\n    '''\n    news = []\n    for i in articles['response']['docs']:\n        dic = {}\n        dic['id'] = i['_id']\n        if i['abstract'] is not None:\n            dic['abstract'] = i['abstract'].encode(\"utf8\")\n        dic['headline'] = i['headline']['main'].encode(\"utf8\")\n        dic['desk'] = i['news_desk']\n        dic['date'] = i['pub_date'][0:10] # cutting time of day.\n        dic['section'] = i['section_name']\n        dic['multimedia'] = i['multimedia']\n        if i['snippet'] is not None:\n            dic['snippet'] = i['snippet'].encode(\"utf8\")\n        dic['source'] = i['source']\n        dic['type'] = i['type_of_material']\n        dic['url'] = i['web_url']\n        dic['word_count'] = i['word_count']\n        # locations\n        locations = []\n        for x in range(0,len(i['keywords'])):\n            if 'glocations' in i['keywords'][x]['name']:\n                locations.append(i['keywords'][x]['value'])\n        dic['locations'] = locations\n        # subject\n        subjects = []\n        for x in range(0,len(i['keywords'])):\n            if 'subject' in i['keywords'][x]['name']:\n                subjects.append(i['keywords'][x]['value'])\n        dic['subjects'] = subjects   \n        news.append(dic)\n    return(news) \ndef get_articles(date,query):\n    '''\n    This function accepts a year in string format (e.g.'1980')\n    and a query (e.g.'Amnesty International') and it will \n    return a list of parsed articles (in dictionaries)\n    for that year.\n    '''\n    all_articles = []\n    for i in range(0,100): #NYT limits pager to first 100 pages. But rarely will you find over 100 pages of results anyway.\n        articles = api.search(q = query,\n               fq = {'source':['Reuters','AP', 'The New York Times']},\n               begin_date = '20140415',\n               end_date = '20140515',\n               sort='oldest',\n               page = str(i))\n        articles = parse_articles(articles)\n        all_articles = all_articles + articles\n    return(all_articles)\n\nAmnesty_all = []\nfor i in range(2014,2015):\n    print 'Processing ' + str(i) + '...'\n    Amnesty_year = get_articles(str(i),'Boko Haram')\n    Amnesty_all = Amnesty_all + Amnesty_year \n\nimport csv\nkeys = Amnesty_all[0].keys()\nwith open('tulokset.csv', 'wb') as output_file:\n    dict_writer = csv.DictWriter(output_file, keys)\n    dict_writer.writeheader()\n    dict_writer.writerows(Amnesty_all)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Processing 2014...\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.9", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}